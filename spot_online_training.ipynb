{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClWs04wqXkVf"
      },
      "outputs": [],
      "source": [
        "#Include this at the top of your colab code\n",
        "import os\n",
        "if not os.path.exists('.mujoco_setup_complete'):\n",
        "  # Get the prereqs\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  # Get Mujoco\n",
        "  !mkdir ~/.mujoco\n",
        "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "  !rm mujoco.tar.gz\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
        "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
        "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "  !ldconfig\n",
        "  # Install Mujoco-py\n",
        "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "  # run once\n",
        "  !touch .mujoco_setup_complete\n",
        "\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/usr/lib/nvidia'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pezPrFXkVk"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyV_gwZCXsls",
        "outputId": "0b2b875a-5707-434f-ca9d-139907826b24"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBLpnaTvXkVl",
        "outputId": "ddde8a14-c25e-4460-dc69-527d437b8833"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions import Normal\n",
        "from typing import Tuple, List, Optional\n",
        "from math import sqrt, log\n",
        "import numpy as np\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from copy import deepcopy\n",
        "import d4rl\n",
        "import gym\n",
        "import random\n",
        "from tqdm import trange\n",
        "from imageio import mimsave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HaMwTP-IXkVl"
      },
      "outputs": [],
      "source": [
        "init_arguments = {\n",
        "    \"hopper-medium-replay\": [11, 3, 6, 1.0],\n",
        "    \"hopper-medium\": [11, 3, 6, 1.0],\n",
        "    \"walker2d-medium-replay\": [17, 6, 12, 1.0],\n",
        "    \"walker2d-medium\": [17, 6, 12, 1.0],\n",
        "    \"halfcheetah-medium-replay\": [17, 6, 12, 1.0],\n",
        "    \"halfcheetah-medium\": [17, 6, 12, 1.0]\n",
        "}\n",
        "\n",
        "def seed_everything(seed: int,\n",
        "                    env: Optional[gym.Env] = None,\n",
        "                    use_deterministic_algos: bool = False):\n",
        "    if env is not None:\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "    \n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.use_deterministic_algorithms(use_deterministic_algos)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CIT2_r4CXkVm"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class spot_config:\n",
        "    save_video: bool = False\n",
        "    buffer_size: int = 1000000\n",
        "    env: str = \"hopper\"  # halfcheetah walker2d hopper\n",
        "    dataset: str = \"medium-replay\"  # medium, medium-replay\n",
        "    version: str = \"v0\"\n",
        "    env_name: str = f\"{env}-{dataset}-{version}\"\n",
        "    seed: int = 0\n",
        "    eval_frequency: int = 5000\n",
        "    max_timesteps: int = 1000000\n",
        "    save_model: bool = False\n",
        "    save_final_model: bool = True\n",
        "    eval_episodes: int = 10\n",
        "    clip: bool = False\n",
        "    exploration_noise: float = 0.1\n",
        "    batch_size: int = 256\n",
        "    discount_factor: float = 0.99\n",
        "    tau: float = 0.005\n",
        "    policy_noise: float = 0.2\n",
        "    noise_clip: float = 0.5\n",
        "    policy_frequency: int = 2\n",
        "    lr: float = 3e-4\n",
        "    actor_lr: float = None\n",
        "    actor_hidden_dim: int = 256\n",
        "    critic_hidden_dim: int = 256\n",
        "    actor_dropout: float = 0.1\n",
        "    alpha: float = 0.4\n",
        "    normalize_env: bool = True\n",
        "    #vae_model_path: str = os.path.join(\"spot\", \"weights\", f\"vae_{env}-{dataset}.pt\")\n",
        "    vae_path: str = f\"vae_{env}-{dataset}.pt\"\n",
        "    beta: float = 0.5\n",
        "    use_importance_sampling: bool = False\n",
        "    num_samples: int = 1\n",
        "    lambda_: float = 1.0\n",
        "    with_q_norm: bool = True\n",
        "    lambda_cool: float = False\n",
        "    lambda_end: float = 0.2\n",
        "    base_dir: str = \"spot\"\n",
        "    weights_dir: str = \"online_policy_weights\"\n",
        "\n",
        "cfg = spot_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wvadXdu-XkVm"
      },
      "outputs": [],
      "source": [
        "#https://github.com/MishaLaskin/rad/blob/master/logger.py\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "FORMAT_CONFIG = {\n",
        "    'rl': {\n",
        "        'train': [\n",
        "            ('episode', 'E', 'int'), ('step', 'S', 'int'),\n",
        "            ('duration', 'D', 'time'), ('episode_reward', 'R', 'float'),\n",
        "            ('batch_reward', 'BR', 'float'), ('actor_loss', 'A_LOSS', 'float'),\n",
        "            ('critic_loss', 'CR_LOSS', 'float')\n",
        "        ],\n",
        "        'eval': [('step', 'S', 'int'), ('episode_reward', 'ER', 'float')]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self._sum = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self._sum += value\n",
        "        self._count += n\n",
        "\n",
        "    def value(self):\n",
        "        return self._sum / max(1, self._count)\n",
        "\n",
        "\n",
        "class MetersGroup(object):\n",
        "    def __init__(self, file_name, formating):\n",
        "        self._file_name = file_name\n",
        "        if os.path.exists(file_name):\n",
        "            os.remove(file_name)\n",
        "        self._formating = formating\n",
        "        self._meters = defaultdict(AverageMeter)\n",
        "\n",
        "    def log(self, key, value, n=1):\n",
        "        self._meters[key].update(value, n)\n",
        "\n",
        "    def _prime_meters(self):\n",
        "        data = dict()\n",
        "        for key, meter in self._meters.items():\n",
        "            if key.startswith('train'):\n",
        "                key = key[len('train') + 1:]\n",
        "            else:\n",
        "                key = key[len('eval') + 1:]\n",
        "            key = key.replace('/', '_')\n",
        "            data[key] = meter.value()\n",
        "        return data\n",
        "\n",
        "    def _dump_to_file(self, data):\n",
        "        with open(self._file_name, 'a') as f:\n",
        "            f.write(json.dumps(data) + '\\n')\n",
        "\n",
        "    def _format(self, key, value, ty):\n",
        "        template = '%s: '\n",
        "        if ty == 'int':\n",
        "            template += '%d'\n",
        "        elif ty == 'float':\n",
        "            template += '%.04f'\n",
        "        elif ty == 'time':\n",
        "            template += '%.01f s'\n",
        "        else:\n",
        "            raise 'invalid format type: %s' % ty\n",
        "        return template % (key, value)\n",
        "\n",
        "    def _dump_to_console(self, data, prefix):\n",
        "        prefix = colored(prefix, 'yellow' if prefix == 'train' else 'green')\n",
        "        pieces = ['{:5}'.format(prefix)]\n",
        "        for key, disp_key, ty in self._formating:\n",
        "            value = data.get(key, 0)\n",
        "            pieces.append(self._format(disp_key, value, ty))\n",
        "        print('| %s' % (' | '.join(pieces)))\n",
        "\n",
        "    def dump(self, step, prefix):\n",
        "        if len(self._meters) == 0:\n",
        "            return\n",
        "        data = self._prime_meters()\n",
        "        data['step'] = step\n",
        "        self._dump_to_file(data)\n",
        "        self._dump_to_console(data, prefix)\n",
        "        self._meters.clear()\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, log_dir, use_tb=True, config='rl'):\n",
        "        self._log_dir = log_dir\n",
        "        if use_tb:\n",
        "            tb_dir = os.path.join(log_dir, 'tb')\n",
        "            if os.path.exists(tb_dir):\n",
        "                shutil.rmtree(tb_dir)\n",
        "            self._sw = SummaryWriter(tb_dir)\n",
        "        else:\n",
        "            self._sw = None\n",
        "        self._train_mg = MetersGroup(\n",
        "            os.path.join(log_dir, 'train.log'),\n",
        "            formating=FORMAT_CONFIG[config]['train']\n",
        "        )\n",
        "        self._eval_mg = MetersGroup(\n",
        "            os.path.join(log_dir, 'eval.log'),\n",
        "            formating=FORMAT_CONFIG[config]['eval']\n",
        "        )\n",
        "\n",
        "    def _try_sw_log(self, key, value, step):\n",
        "        if self._sw is not None:\n",
        "            self._sw.add_scalar(key, value, step)\n",
        "\n",
        "    def _try_sw_log_image(self, key, image, step):\n",
        "        if self._sw is not None:\n",
        "            assert image.dim() == 3\n",
        "            grid = torchvision.utils.make_grid(image.unsqueeze(1))\n",
        "            self._sw.add_image(key, grid, step)\n",
        "\n",
        "    def _try_sw_log_video(self, key, frames, step):\n",
        "        if self._sw is not None:\n",
        "            frames = torch.from_numpy(np.array(frames))\n",
        "            frames = frames.unsqueeze(0)\n",
        "            self._sw.add_video(key, frames, step, fps=30)\n",
        "\n",
        "    def _try_sw_log_histogram(self, key, histogram, step):\n",
        "        if self._sw is not None:\n",
        "            self._sw.add_histogram(key, histogram, step)\n",
        "\n",
        "    def log(self, key, value, step, n=1):\n",
        "        assert key.startswith('train') or key.startswith('eval')\n",
        "        if type(value) == torch.Tensor:\n",
        "            value = value.item()\n",
        "        self._try_sw_log(key, value / n, step)\n",
        "        mg = self._train_mg if key.startswith('train') else self._eval_mg\n",
        "        mg.log(key, value, n)\n",
        "\n",
        "    def log_param(self, key, param, step):\n",
        "        self.log_histogram(key + '_w', param.weight.data, step)\n",
        "        if hasattr(param.weight, 'grad') and param.weight.grad is not None:\n",
        "            self.log_histogram(key + '_w_g', param.weight.grad.data, step)\n",
        "        if hasattr(param, 'bias'):\n",
        "            self.log_histogram(key + '_b', param.bias.data, step)\n",
        "            if hasattr(param.bias, 'grad') and param.bias.grad is not None:\n",
        "                self.log_histogram(key + '_b_g', param.bias.grad.data, step)\n",
        "\n",
        "    def log_image(self, key, image, step):\n",
        "        assert key.startswith('train') or key.startswith('eval')\n",
        "        self._try_sw_log_image(key, image, step)\n",
        "\n",
        "    def log_video(self, key, frames, step):\n",
        "        assert key.startswith('train') or key.startswith('eval')\n",
        "        self._try_sw_log_video(key, frames, step)\n",
        "\n",
        "    def log_histogram(self, key, histogram, step):\n",
        "        assert key.startswith('train') or key.startswith('eval')\n",
        "        self._try_sw_log_histogram(key, histogram, step)\n",
        "\n",
        "    def dump(self, step):\n",
        "        self._train_mg.dump(step, 'train')\n",
        "        self._eval_mg.dump(step, 'eval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NRfeToQvXkVo"
      },
      "outputs": [],
      "source": [
        "def make_dir(dir_path):\n",
        "    try:\n",
        "        os.mkdir(dir_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    return dir_path\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 buffer_size: int = 1000000) -> None:\n",
        "        self.buffer_size = buffer_size\n",
        "        self.pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        self.states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
        "        self.actions = torch.zeros((buffer_size, action_dim), dtype=torch.float32, device=device)\n",
        "        self.rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
        "        self.next_states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
        "        self.dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "        # i/o order: state, action, reward, next_state, done\n",
        "    \n",
        "    def from_json(self, json_file: str):\n",
        "        import json\n",
        "\n",
        "        if not json_file.endswith('.json'):\n",
        "            json_file = json_file + '.json'\n",
        "\n",
        "        json_file = os.path.join(\"json_datasets\", json_file)\n",
        "        output = dict()\n",
        "\n",
        "        with open(json_file) as f:\n",
        "            dataset = json.load(f)\n",
        "        \n",
        "        for k, v in dataset.items():\n",
        "            v = np.array(v)\n",
        "            if k != \"terminals\":\n",
        "                v = v.astype(np.float32)\n",
        "            \n",
        "            output[k] = v\n",
        "        \n",
        "        self.from_d4rl(output)\n",
        "    \n",
        "    @staticmethod\n",
        "    def to_tensor(data: np.ndarray, device=None) -> torch.Tensor:\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    \n",
        "    def sample(self, batch_size: int):\n",
        "        indexes = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            self.to_tensor(self.states[indexes], self.device),\n",
        "            self.to_tensor(self.actions[indexes], self.device),\n",
        "            self.to_tensor(self.rewards[indexes], self.device),\n",
        "            self.to_tensor(self.next_states[indexes], self.device),\n",
        "            self.to_tensor(self.dones[indexes], self.device)\n",
        "        )\n",
        "    \n",
        "    def from_d4rl(self, dataset):\n",
        "        if self.size:\n",
        "            print(\"Warning: loading data into non-empty buffer\")\n",
        "        n_transitions = dataset[\"observations\"].shape[0]\n",
        "\n",
        "        if n_transitions < self.buffer_size:\n",
        "            self.states[:n_transitions] = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
        "            self.actions[:n_transitions] = self.to_tensor(dataset[\"actions\"][-n_transitions:], self.device)\n",
        "            self.next_states[:n_transitions] = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
        "            self.rewards[:n_transitions] = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "            self.dones[:n_transitions] = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "\n",
        "        else:\n",
        "            self.buffer_size = n_transitions\n",
        "\n",
        "            self.states = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
        "            self.actions = self.to_tensor(dataset[\"actions\"][-n_transitions:])\n",
        "            self.next_states = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
        "            self.rewards = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "            self.dones = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "        \n",
        "        self.size = n_transitions\n",
        "        self.pointer = n_transitions % self.buffer_size\n",
        "    \n",
        "    def from_d4rl_finetune(self, dataset):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def normalize_states(self, eps=1e-3):\n",
        "        mean = self.states.mean(0, keepdim=True)\n",
        "        std = self.states.std(0, keepdim=True) + eps\n",
        "        self.states = (self.states - mean) / std\n",
        "        self.next_states = (self.next_states - mean) / std\n",
        "        return mean, std\n",
        "    \n",
        "    def clip(self, eps=1e-5):\n",
        "        self.action = torch.clip(self.action, - 1 + eps, 1 - eps)\n",
        "\n",
        "    def add_transition(self,\n",
        "                       state: torch.Tensor,\n",
        "                       action: torch.Tensor,\n",
        "                       reward: torch.Tensor,\n",
        "                       next_state: torch.Tensor,\n",
        "                       done: torch.Tensor):\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = self.to_tensor(state)\n",
        "            action = self.to_tensor(action)\n",
        "            reward = self.to_tensor(reward)\n",
        "            next_state = self.to_tensor(next_state)\n",
        "            done = self.to_tensor(done)\n",
        "\n",
        "\n",
        "        self.states[self.pointer] = state\n",
        "        self.actions[self.pointer] = action\n",
        "        self.rewards[self.pointer] = reward\n",
        "        self.next_states[self.pointer] = next_state\n",
        "        self.dones[self.pointer] = done\n",
        "\n",
        "        self.pointer = (self.pointer + 1) % self.buffer_size\n",
        "        self.size = min(self.size + 1, self.buffer_size)\n",
        "    \n",
        "    def add_batch(self,\n",
        "                  states: List[torch.Tensor],\n",
        "                  actions: List[torch.Tensor],\n",
        "                  rewards: List[torch.Tensor],\n",
        "                  next_states: List[torch.Tensor],\n",
        "                  dones: List[torch.Tensor]):\n",
        "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
        "            self.add_transition(state, action, reward, next_state, done)\n",
        "    \n",
        "    @staticmethod\n",
        "    def dataset_stats(dataset):\n",
        "        episode_returns = []\n",
        "        returns = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        for reward, done in zip(dataset[\"rewards\"], dataset[\"terminals\"]):\n",
        "            if done:\n",
        "                episode_returns.append(returns)\n",
        "                returns = 0\n",
        "                episode_length = 0\n",
        "            else:\n",
        "                episode_length += 1\n",
        "                returns += reward\n",
        "                if episode_length == 1000:\n",
        "                    episode_returns.append(returns)\n",
        "                    returns = 0\n",
        "                    episode_length = 0\n",
        "\n",
        "        episode_returns = np.array(episode_returns)\n",
        "        return episode_returns.mean(), episode_returns.std()\n",
        "\n",
        "\n",
        "def train_val_split(replay_buffer: ReplayBuffer, val_size: float) -> Tuple[ReplayBuffer, ReplayBuffer]:\n",
        "    data_size = replay_buffer.size\n",
        "    val_size = int(data_size * val_size)\n",
        "\n",
        "    permutation = torch.randperm(data_size)\n",
        "        \n",
        "    train_rb = ReplayBuffer(replay_buffer.state_dim, replay_buffer.action_dim)\n",
        "    val_rb = ReplayBuffer(replay_buffer.state_dim, replay_buffer.action_dim)\n",
        "\n",
        "    train_rb.add_batch(\n",
        "        replay_buffer.states[permutation[val_size:]],\n",
        "        replay_buffer.actions[permutation[val_size:]],\n",
        "        replay_buffer.rewards[permutation[val_size:]],\n",
        "        replay_buffer.next_states[permutation[val_size:]],\n",
        "        replay_buffer.dones[permutation[val_size:]]\n",
        "        )\n",
        "\n",
        "    val_rb.add_batch(\n",
        "        replay_buffer.states[permutation[:val_size]],\n",
        "        replay_buffer.actions[permutation[:val_size]],\n",
        "        replay_buffer.rewards[permutation[:val_size]],\n",
        "        replay_buffer.next_states[permutation[:val_size]],\n",
        "        replay_buffer.dones[permutation[:val_size]]\n",
        "        )\n",
        "        \n",
        "    return train_rb, val_rb\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 max_action: float = None,\n",
        "                 dropout: float = None,\n",
        "                 hidden_dim: int = 256,\n",
        "                 uniform_initialization: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if dropout is None:\n",
        "            dropout = 0\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        action = self.actor(state)\n",
        "\n",
        "        if self.max_action is not None:\n",
        "            return self.max_action * torch.tanh(action)\n",
        "        return action\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 hidden_dim: int = 256,\n",
        "                 uniform_initialization: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.q1_ = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        self.q2_ = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self,\n",
        "                state: torch.Tensor,\n",
        "                action: torch.Tensor):\n",
        "        concat = torch.cat([state, action], 1)\n",
        "\n",
        "        return self.q1_(concat), self.q2_(concat)\n",
        "    \n",
        "    def q1(self,\n",
        "           state: torch.Tensor,\n",
        "           action: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        return self.q1_(torch.cat([state, action], 1))\n",
        "\n",
        "\n",
        "class ConditionalVAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 latent_dim: int,\n",
        "                 max_action: int = None,\n",
        "                 hidden_dim: int = 750,\n",
        "                 device: torch.device = None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "        self.max_action = max_action\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.e1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.e2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.log_std = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        self.d1 = nn.Linear(state_dim + latent_dim, hidden_dim)\n",
        "        self.d2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.d3 = nn.Linear(hidden_dim, action_dim)\n",
        "    \n",
        "    def encode(self,\n",
        "               state: torch.Tensor,\n",
        "               action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \n",
        "        z = F.relu(self.e1(torch.cat([state, action], -1)))\n",
        "        z = F.relu(self.e2(z))\n",
        "\n",
        "        mean = self.mean(z)\n",
        "        std = torch.exp(self.log_std(z).clamp(-4, 15))  # see __ in 'paper' folder\n",
        "        return mean, std\n",
        "    \n",
        "    def decode(self,\n",
        "               state: torch.Tensor,\n",
        "               z: torch.Tensor = None) -> torch.Tensor:\n",
        "        \n",
        "        if z is None:\n",
        "            z = torch.randn((state.shape[0], self.latent_dim)).to(self.device).clamp(-0.5, 0.5)  # see __ in 'paper' folder\n",
        "        \n",
        "        action = F.relu(self.d1(torch.cat([state, z], -1)))\n",
        "        action = F.relu(self.d2(action))\n",
        "        action = self.d3(action)\n",
        "\n",
        "        if self.max_action is not None:\n",
        "            return self.max_action * torch.tanh(action)\n",
        "        return action\n",
        "    \n",
        "    def forward(self,\n",
        "                state: torch.Tensor,\n",
        "                action: torch.Tensor):\n",
        "        \n",
        "        mean, std = self.encode(state, action)\n",
        "        z = mean * std * torch.randn_like(std)\n",
        "\n",
        "        return self.decode(state, z), mean, std\n",
        "    \n",
        "    def importance_sampling_loss(self,\n",
        "                                 state: torch.Tensor,\n",
        "                                 action: torch.Tensor,\n",
        "                                 beta: float,\n",
        "                                 num_samples: int = 10) -> torch.Tensor:\n",
        "        # see eq8 in 'paper' folder\n",
        "        mean, std = self.encode(state, action)\n",
        "        \n",
        "        mean = mean.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        std = std.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        z = mean + std * torch.randn_like(std)\n",
        "        state = state.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        action = action.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        \n",
        "        mean_decoded = self.decode(state, z)\n",
        "        scale_factor = sqrt(beta) / 2\n",
        "\n",
        "        log_prob_q_zx = Normal(loc=mean, scale=std).log_prob(z)\n",
        "        mean_prior = torch.zeros_like(z).to(self.device)\n",
        "        std_prior = torch.ones_like(z).to(self.device)\n",
        "        log_prob_p_z = Normal(loc=mean_prior, scale=std_prior).log_prob(z)\n",
        "        std_decoded = torch.ones_like(mean_decoded).to(self.device) * scale_factor\n",
        "        log_prob_p_xz = Normal(loc=mean_decoded, scale=std_decoded).log_prob(action)\n",
        "\n",
        "        w = log_prob_p_xz.sum(-1) + log_prob_p_z.sum(-1) - log_prob_q_zx.sum(-1)\n",
        "        score = w.logsumexp(dim=-1) - log(num_samples)\n",
        "        return -score\n",
        "    \n",
        "    def elbo_loss(self,\n",
        "                  state: torch.Tensor,\n",
        "                  action: torch.Tensor,\n",
        "                  beta: float,\n",
        "                  num_samples: int = 10) -> torch.Tensor:\n",
        "        # see eq7 in 'paper' folder\n",
        "        mean, std = self.encode(state, action)\n",
        "\n",
        "        mean = mean.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        std = std.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        z = mean + std * torch.randn_like(std)\n",
        "        state = state.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "        action = action.repeat(num_samples, 1, 1).permute(1, 0, 2)\n",
        "\n",
        "        decoded = self.decode(state, z)\n",
        "        reconstruction_loss = ((decoded - action) ** 2).mean(dim=(1, 2))\n",
        "\n",
        "        kl_loss = -1 / 2 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean(-1)\n",
        "        loss = reconstruction_loss + beta * kl_loss\n",
        "        return loss\n",
        "    \n",
        "    def load(self, filename):\n",
        "        self.load_state_dict(torch.load(filename, map_location=self.device))\n",
        "\n",
        "\n",
        "class SPOT:\n",
        "    diverging_threshold = 1e4\n",
        "\n",
        "    def __init__(self,\n",
        "                 vae: ConditionalVAE,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 max_action: float = None,\n",
        "                 discount_factor: float = 0.99,\n",
        "                 tau: float = 0.005,\n",
        "                 policy_noise: float = 0.2,\n",
        "                 noise_clip: float = 0.5,\n",
        "                 policy_frequency: int = 2,\n",
        "                 beta: float = 0.5,\n",
        "                 lambda_: float = 1.0,\n",
        "                 lr: float = 3e-4,\n",
        "                 actor_lr: float = None,\n",
        "                 with_q_norm: bool = True,\n",
        "                 num_samples: int = 1,\n",
        "                 use_importance_sampling: bool = False,\n",
        "                 actor_hidden_dim: int = 256,\n",
        "                 critic_hidden_dim: int = 256,\n",
        "                 actor_dropout: float = 0.1,\n",
        "                 actor_init_w: bool = False,\n",
        "                 critic_init_w: bool = False,\n",
        "                 lambda_cool: bool = False,\n",
        "                 lambda_end: float = 0.2) -> None:\n",
        "        \n",
        "        self.iterations = 0\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim, max_action, actor_dropout, actor_hidden_dim, actor_init_w).to(device)\n",
        "        self.actor_target = deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr or actor_lr)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim, critic_hidden_dim, critic_init_w).to(device)\n",
        "        self.critic_target = deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "        self.discount_factor = discount_factor\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_frequency = policy_frequency\n",
        "        self.vae = vae\n",
        "        self.beta = beta\n",
        "        self.num_samples = num_samples\n",
        "        self.use_importance_sampling = use_importance_sampling\n",
        "        self.with_q_norm = with_q_norm\n",
        "        self.lambda_ = lambda_\n",
        "        self.lambda_cool = lambda_cool\n",
        "        self.lambda_end = lambda_end\n",
        "    \n",
        "    @staticmethod\n",
        "    def to_tensor(data, device=None) -> torch.Tensor:\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def act(self, state: np.ndarray) -> np.ndarray:\n",
        "        self.actor.eval()\n",
        "        state = self.to_tensor(state.reshape(1, -1), device=self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        self.actor.train()\n",
        "        return action\n",
        "\n",
        "    def soft_update(self, regime):\n",
        "        if regime == \"actor\":\n",
        "            for param, tgt_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
        "        else:\n",
        "            for param, tgt_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
        "    \n",
        "    def train(self,\n",
        "              replay_buffer: ReplayBuffer,\n",
        "              batch_size: int = 256,\n",
        "              logger: Logger = None) -> None:\n",
        "        self.iterations += 1\n",
        "        \n",
        "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            tgt_q1, tgt_q2 = self.critic_target(next_state, next_action)\n",
        "            tgt_q = torch.min(tgt_q1, tgt_q2)\n",
        "\n",
        "            tgt_q = reward + (1 - done) * self.discount_factor * tgt_q  # eq1 in 'paper' folder\n",
        "        \n",
        "        current_q1, current_q2 = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(current_q1, tgt_q) + F.mse_loss(current_q2, tgt_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log(\"train/critic_loss\", critic_loss, self.iterations)\n",
        "        \n",
        "        if not self.iterations % self.policy_frequency:\n",
        "            \n",
        "            pi = self.actor(state)\n",
        "            q = self.critic.q1(state, pi)\n",
        "            \n",
        "            if self.use_importance_sampling:\n",
        "                density_estimator_loss = self.vae.importance_sampling_loss(state, pi, self.beta, self.num_samples)\n",
        "            else:\n",
        "                density_estimator_loss = self.vae.elbo_loss(state, pi, self.beta, self.num_samples)\n",
        "            \n",
        "            # see practical_algo.jpeg in 'paper' folder\n",
        "            if self.with_q_norm:\n",
        "                actor_loss = -q.mean() / q.abs().mean().detach() + self.lambda_ * density_estimator_loss.mean()\n",
        "            else:\n",
        "                actor_loss = -q.mean() + self.lambda_ * density_estimator_loss.mean()\n",
        "            \n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            if logger is not None:\n",
        "                logger.log(\"train/Q\", q.mean(), self.iterations)\n",
        "                logger.log(\"train/actor_loss\", actor_loss, self.iterations)\n",
        "                logger.log(\"train/neg_log_beta\", density_estimator_loss.mean(), self.iterations)\n",
        "                logger.log(\"train/neg_log_beta_max\", density_estimator_loss.max(), self.iterations)\n",
        "            \n",
        "            if q.mean().item() > self.diverging_threshold:\n",
        "                exit()\n",
        "            \n",
        "            self.soft_update(regime=\"actor\")\n",
        "            self.soft_update(regime=\"critic\")\n",
        "\n",
        "    def train_online(self,\n",
        "                     replay_buffer: ReplayBuffer,\n",
        "                     batch_size: int = 256,\n",
        "                     logger: Logger =None) -> None:\n",
        "        self.iterations += 1\n",
        "        \n",
        "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            tgt_q1, tgt_q2 = self.critic_target(next_state, next_action)\n",
        "            tgt_q = torch.min(tgt_q1, tgt_q2)\n",
        "\n",
        "            tgt_q = reward + (1 - done) * self.discount_factor * tgt_q\n",
        "        \n",
        "        current_q1, current_q2 = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(current_q1, tgt_q) + F.mse_loss(current_q2, tgt_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log(\"train/critic_loss\", critic_loss, self.iterations)\n",
        "        \n",
        "        if not self.iterations % self.policy_frequency:\n",
        "            \n",
        "            pi = self.actor(state)\n",
        "            q = self.critic.q1(state, pi)\n",
        "            \n",
        "            if self.use_importance_sampling:\n",
        "                density_estimator_loss = self.vae.importance_sampling_loss(state, pi, self.beta, self.num_samples)\n",
        "            else:\n",
        "                density_estimator_loss = self.vae.elbo_loss(state, pi, self.beta, self.num_samples)\n",
        "            \n",
        "            # additional component for online learning\n",
        "            lambda_ = self.lambda_\n",
        "            if self.lambda_cool:\n",
        "                lambda_ = self.lambda_ * max(self.lambda_end, (1.0 - self.iterations / 1000000))\n",
        "\n",
        "                if logger is not None:\n",
        "                    logger.log(\"train/lambda_\", lambda_, self.iterations)\n",
        "            \n",
        "            if self.with_q_norm:\n",
        "                actor_loss = -q.mean() / q.abs().mean().detach() + lambda_ * density_estimator_loss.mean()\n",
        "            else:\n",
        "                actor_loss = -q.mean() + lambda_ * density_estimator_loss.mean()\n",
        "            \n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            if logger is not None:\n",
        "                logger.log(\"train/Q\", q.mean(), self.iterations)\n",
        "                logger.log(\"train/actor_loss\", actor_loss, self.iterations)\n",
        "                logger.log(\"train/neg_log_beta\", density_estimator_loss.mean(), self.iterations)\n",
        "                logger.log(\"train/neg_log_beta_max\", density_estimator_loss.max(), self.iterations)\n",
        "            \n",
        "\n",
        "            self.soft_update(regime=\"actor\")\n",
        "            self.soft_update(regime=\"critic\")\n",
        "\n",
        "    def save(self, model_dir):\n",
        "        make_dir(model_dir)\n",
        "\n",
        "        torch.save(self.critic.state_dict(), os.path.join(model_dir, f\"critic_s{str(self.iterations)}.pth\"))\n",
        "        torch.save(self.critic_target.state_dict(), os.path.join(model_dir, f\"critic_target_s{str(self.iterations)}.pth\"))\n",
        "        torch.save(self.critic_optimizer.state_dict(), os.path.join(\n",
        "            model_dir, f\"critic_optimizer_s{str(self.iterations)}.pth\"))\n",
        "\n",
        "        torch.save(self.actor.state_dict(), os.path.join(model_dir, f\"actor_s{str(self.iterations)}.pth\"))\n",
        "        torch.save(self.actor_target.state_dict(), os.path.join(model_dir, f\"actor_target_s{str(self.iterations)}.pth\"))\n",
        "        torch.save(self.actor_optimizer.state_dict(), os.path.join(\n",
        "            model_dir, f\"actor_optimizer_s{str(self.iterations)}.pth\"))\n",
        "\n",
        "    def load(self, model_dir, step=1000000):\n",
        "        self.critic.load_state_dict(torch.load(os.path.join(model_dir, f\"critic_s{str(step)}.pth\")))\n",
        "        self.critic_target.load_state_dict(torch.load(os.path.join(model_dir, f\"critic_target_s{str(step)}.pth\")))\n",
        "        self.critic_optimizer.load_state_dict(torch.load(os.path.join(model_dir, f\"critic_optimizer_s{str(step)}.pth\")))\n",
        "\n",
        "        self.actor.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_s{str(step)}.pth\")))\n",
        "        self.actor_target.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_target_s{str(step)}.pth\")))\n",
        "        self.actor_optimizer.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_optimizer_s{str(step)}.pth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FgdMHj2MXkVr"
      },
      "outputs": [],
      "source": [
        "class VideoRecorder:\n",
        "    def __init__(self, dir_name, height=512, width=512, camera_id=0, fps=60):\n",
        "        self.dir_name = dir_name\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.camera_id = camera_id\n",
        "        self.fps = fps\n",
        "        self.frames = []\n",
        "\n",
        "    def init(self, enabled=True):\n",
        "        self.frames = []\n",
        "        self.enabled = self.dir_name is not None and enabled\n",
        "\n",
        "    def record(self, env: gym.Env):\n",
        "        if self.enabled:\n",
        "            frame = env.render(\n",
        "                mode='rgb_array',\n",
        "                height=self.height,\n",
        "                width=self.width,\n",
        "                # camera_id=self.camera_id\n",
        "            )\n",
        "            self.frames.append(frame)\n",
        "\n",
        "    def save(self, file_name):\n",
        "        if self.enabled:\n",
        "            path = os.path.join(self.dir_name, file_name)\n",
        "            mimsave(path, self.frames, fps=self.fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BJfjnBtOXkVs"
      },
      "outputs": [],
      "source": [
        "def eval_policy(cfg: spot_config,\n",
        "                iteration: int,\n",
        "                recorder: VideoRecorder,\n",
        "                logger: Logger,\n",
        "                policy: SPOT,\n",
        "                env_name: str,\n",
        "                seed: int,\n",
        "                mean: np.ndarray,\n",
        "                std: np.ndarray,\n",
        "                eval_episodes: int = 10):\n",
        "    env = gym.make(env_name)\n",
        "    env.seed(seed + 100)\n",
        "\n",
        "    lengths, returns, last_rewards = [], [], []\n",
        "    average_reward = 0.0\n",
        "\n",
        "    for episode in trange(eval_episodes):\n",
        "        recorder.init(enabled=cfg.save_video)\n",
        "        state, done = env.reset(), False\n",
        "        \n",
        "        #recorder.record(env)\n",
        "        steps = 0\n",
        "        episode_return = 0\n",
        "\n",
        "        while not done:\n",
        "            state = (np.array(state).reshape(1, -1) - mean) / std\n",
        "            action = policy.act(state)\n",
        "\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            recorder.record(env)\n",
        "\n",
        "            average_reward += reward\n",
        "            episode_return += reward\n",
        "            steps += 1\n",
        "\n",
        "        lengths.append(steps)\n",
        "        returns.append(episode_return)\n",
        "        last_rewards.append(reward)\n",
        "        recorder.save(f\"evaluation_{iteration}_episode{episode}_return_{episode_return}.mp4\")\n",
        "    \n",
        "    average_reward /= eval_episodes\n",
        "    d4rl_score = env.get_normalized_score(average_reward)\n",
        "\n",
        "    if logger is not None:\n",
        "        logger.log('eval/lengths_mean', np.mean(lengths), iteration)\n",
        "        logger.log('eval/lengths_std', np.std(lengths), iteration)\n",
        "        logger.log('eval/returns_mean', np.mean(returns), iteration)\n",
        "        logger.log('eval/returns_std', np.std(returns), iteration)\n",
        "        logger.log('eval/d4rl_score', d4rl_score, iteration)\n",
        "    \n",
        "    return d4rl_score\n",
        "\n",
        "\n",
        "def train_policy(cfg=spot_config()):\n",
        "    video_dir = os.path.join(cfg.base_dir, \"video\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    weights_dir = os.path.join(cfg.base_dir, cfg.weights_dir)\n",
        "    make_dir(weights_dir)\n",
        "    make_dir(video_dir)\n",
        "\n",
        "    env = gym.make(cfg.env_name)\n",
        "\n",
        "    seed_everything(cfg.seed)\n",
        "\n",
        "    \n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    \n",
        "    #state_dim, action_dim, max_action = parse_json_dataset(cfg.env_name)\n",
        "\n",
        "    vae = ConditionalVAE(state_dim, action_dim, action_dim * 2, max_action).to(device)\n",
        "    !wget \"https://github.com/zzmtsvv/rl_task/raw/main/spot/vae_weights/{cfg.vae_path}\"\n",
        "    vae.load(cfg.vae_path)\n",
        "    vae.eval()\n",
        "\n",
        "    policy = SPOT(vae,\n",
        "                  state_dim=state_dim,\n",
        "                  action_dim=action_dim,\n",
        "                  max_action=max_action,\n",
        "                  discount_factor=cfg.discount_factor,\n",
        "                  tau=cfg.tau,\n",
        "                  policy_noise=cfg.policy_noise,\n",
        "                  noise_clip=cfg.noise_clip,\n",
        "                  policy_frequency=cfg.policy_frequency,\n",
        "                  beta=cfg.beta,\n",
        "                  lambda_=cfg.lambda_,\n",
        "                  lr=cfg.lr,\n",
        "                  actor_lr=cfg.actor_lr,\n",
        "                  with_q_norm=cfg.with_q_norm,\n",
        "                  num_samples=cfg.num_samples,\n",
        "                  use_importance_sampling=cfg.use_importance_sampling,\n",
        "                  actor_hidden_dim=cfg.actor_hidden_dim,\n",
        "                  actor_dropout=cfg.actor_dropout)\n",
        "    \n",
        "    model_dir = os.path.join(\"spot_weights\", f\"{cfg.env}_policy_weights\")\n",
        "    policy.load(model_dir)\n",
        "    \n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim, buffer_size=cfg.buffer_size)\n",
        "    replay_buffer.from_d4rl(d4rl.qlearning_dataset(env))\n",
        "    #replay_buffer.from_json(cfg.env_name)\n",
        "    #assert replay_buffer.size + cfg.max_timesteps <= replay_buffer.buffer_size\n",
        "\n",
        "    mean, std = 0, 1\n",
        "    if cfg.normalize_env:\n",
        "        mean, std = replay_buffer.normalize_states()\n",
        "        mean, std = mean.cpu().numpy(), std.cpu().numpy()\n",
        "    \n",
        "    if cfg.clip:\n",
        "        replay_buffer.clip()\n",
        "    \n",
        "    logger = Logger(os.path.join(cfg.base_dir, \"runs\"), use_tb=True)\n",
        "    recorder = VideoRecorder(video_dir)\n",
        "\n",
        "    for timestep in trange(cfg.max_timesteps):\n",
        "        policy.train_online(replay_buffer, batch_size=cfg.batch_size, logger=logger)\n",
        "\n",
        "        if not (timestep + 1) % cfg.eval_frequency:\n",
        "            d4rl_score = eval_policy(cfg,\n",
        "                                     timestep + 1,\n",
        "                                     recorder,\n",
        "                                     logger,\n",
        "                                     policy,\n",
        "                                     cfg.env_name,\n",
        "                                     cfg.seed,\n",
        "                                     mean,\n",
        "                                     std,\n",
        "                                     cfg.eval_episodes)\n",
        "            \n",
        "            if cfg.save_model:\n",
        "                policy.save(weights_dir)\n",
        "    \n",
        "    if cfg.save_final_model:\n",
        "        policy.save(weights_dir)\n",
        "    \n",
        "    logger._sw.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVkduIP7ZT08",
        "outputId": "c23439c9-4355-4045-f98a-930ccd3bb790"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/spot_weights.zip /content/\n",
        "!unzip -u /content/spot_weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qCoiuGPZq1F",
        "outputId": "b7ac2edc-72f0-435a-98b0-90f2666f4183"
      },
      "outputs": [],
      "source": [
        "train_policy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
