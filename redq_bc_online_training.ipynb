{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JourAaw7dpCC",
        "outputId": "4b76c665-e736-4cb1-fe9e-a5f1290a4c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libgl1-mesa-glx_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../01-libgles1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../02-libgles-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../03-libopengl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../04-libglvnd-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../05-libgl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Selecting previously unselected package libglew2.1:amd64.\n",
            "Preparing to unpack .../06-libglew2.1_2.1.0-4_amd64.deb ...\n",
            "Unpacking libglew2.1:amd64 (2.1.0-4) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../07-libglew-dev_2.1.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.1.0-4) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../08-libglfw3_3.3.2-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.3.2-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../09-patchelf_0.10-2build1_amd64.deb ...\n",
            "Unpacking patchelf (0.10-2build1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../10-libosmesa6_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../11-libosmesa6-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Setting up libglfw3:amd64 (3.3.2-1) ...\n",
            "Setting up libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Setting up libglew2.1:amd64 (2.1.0-4) ...\n",
            "Setting up libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Setting up libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Setting up patchelf (0.10-2build1) ...\n",
            "Setting up libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Setting up libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
            "Setting up libglew-dev:amd64 (2.1.0-4) ...\n",
            "Setting up libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mujoco-py<2.2,>=2.1\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners~=0.15\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1) (1.15.1)\n",
            "Collecting glfw>=1.4.0\n",
            "  Downloading glfw-2.5.9-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1) (0.29.34)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1) (1.22.4)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1) (2.25.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (8.4.0)\n",
            "Installing collected packages: glfw, fasteners, mujoco-py\n",
            "Successfully installed fasteners-0.18 glfw-2.5.9 mujoco-py-2.1.2.14\n",
            "Compiling /usr/local/lib/python3.9/dist-packages/mujoco_py/cymj.pyx because it depends on /usr/local/lib/python3.9/dist-packages/mujoco_py/pxd/mujoco.pxd.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.9/dist-packages/mujoco_py/cymj.pyx\n",
            "running build_ext\n",
            "building 'mujoco_py.cymj' extension\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py/gl\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.9/dist-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/usr/local/lib/python3.9/dist-packages/numpy/core/include -I/usr/include/python3.9 -c /usr/local/lib/python3.9/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.9/dist-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/usr/local/lib/python3.9/dist-packages/numpy/core/include -I/usr/include/python3.9 -c /usr/local/lib/python3.9/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/lib.linux-x86_64-3.9\n",
            "creating /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/lib.linux-x86_64-3.9/mujoco_py\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/temp.linux-x86_64-3.9/usr/local/lib/python3.9/dist-packages/mujoco_py/gl/osmesashim.o -L/root/.mujoco/mujoco210/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.9/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_39_linuxcpuextensionbuilder/lib.linux-x86_64-3.9/mujoco_py/cymj.cpython-39-x86_64-linux-gnu.so -fopenmp\n"
          ]
        }
      ],
      "source": [
        "#Include this at the top of your colab code\n",
        "import os\n",
        "if not os.path.exists('.mujoco_setup_complete'):\n",
        "  # Get the prereqs\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  # Get Mujoco\n",
        "  !mkdir ~/.mujoco\n",
        "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "  !rm mujoco.tar.gz\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
        "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
        "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "  !ldconfig\n",
        "  # Install Mujoco-py\n",
        "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "  # run once\n",
        "  !touch .mujoco_setup_complete\n",
        "\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/usr/lib/nvidia'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnHxr8QTtRPg",
        "outputId": "1d1bcf57-ac9a-491e-9a04-48af6275b506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/redq_bc_weights.zip /content/\n",
        "!unzip -u /content/redq_bc_weights.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ1ikl6btUuT",
        "outputId": "7a23190f-902d-435f-e96a-9ad65c303a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/redq_bc_weights.zip\n",
            "   creating: redq_bc_weights/\n",
            "  inflating: __MACOSX/._redq_bc_weights  \n",
            "  inflating: redq_bc_weights/redq_bc_hopper-medium-replay-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_hopper-medium-replay-v0_42_policy.pth  \n",
            "  inflating: redq_bc_weights/redq_bc_halfcheetah-medium-replay-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_halfcheetah-medium-replay-v0_42_policy.pth  \n",
            "  inflating: redq_bc_weights/redq_bc_halfcheetah-medium-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_halfcheetah-medium-v0_42_policy.pth  \n",
            "  inflating: redq_bc_weights/redq_bc_walker2d-medium-replay-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_walker2d-medium-replay-v0_42_policy.pth  \n",
            "  inflating: redq_bc_weights/redq_bc_walker2d-medium-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_walker2d-medium-v0_42_policy.pth  \n",
            "  inflating: redq_bc_weights/redq_bc_hopper-medium-v0_42_policy.pth  \n",
            "  inflating: __MACOSX/redq_bc_weights/._redq_bc_hopper-medium-v0_42_policy.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKr12fENdpCF",
        "outputId": "f782a54d-19dd-4a3b-97e8-4ebb9a3615d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d4rl\n",
            "  Cloning https://github.com/tinkoff-ai/d4rl (to revision master) to /tmp/pip-install-1xd_k7mm/d4rl_ff47f5c7c458417ca5b8744d07d019c8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tinkoff-ai/d4rl /tmp/pip-install-1xd_k7mm/d4rl_ff47f5c7c458417ca5b8744d07d019c8\n",
            "  Resolved https://github.com/tinkoff-ai/d4rl to commit db6e4b34bb5ce2a51dd3879177c0a0223208a614\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl\n",
            "  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-1xd_k7mm/mjrl_8d225face0104912821608a9029706c2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/aravindr93/mjrl /tmp/pip-install-1xd_k7mm/mjrl_8d225face0104912821608a9029706c2\n",
            "  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym<0.24.0\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from d4rl) (1.22.4)\n",
            "Requirement already satisfied: mujoco_py in /usr/local/lib/python3.9/dist-packages (from d4rl) (2.1.2.14)\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from d4rl) (3.8.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from d4rl) (2.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from d4rl) (8.1.3)\n",
            "Collecting dm_control>=1.0.3\n",
            "  Downloading dm_control-1.0.11-py3-none-any.whl (39.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (4.9.2)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (4.65.0)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (0.1.8)\n",
            "Collecting dm-env\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (2.27.1)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (67.6.1)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (3.1.6)\n",
            "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (1.10.1)\n",
            "Collecting mujoco>=2.3.3\n",
            "  Downloading mujoco-2.3.3-2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting labmaze\n",
            "  Downloading labmaze-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: glfw in /usr/local/lib/python3.9/dist-packages (from dm_control>=1.0.3->d4rl) (2.5.9)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym<0.24.0->d4rl) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym<0.24.0->d4rl) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from gym<0.24.0->d4rl) (6.1.0)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.9/dist-packages (from mujoco_py->d4rl) (1.15.1)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.9/dist-packages (from mujoco_py->d4rl) (0.29.34)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.9/dist-packages (from mujoco_py->d4rl) (0.18)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.9/dist-packages (from mujoco_py->d4rl) (2.25.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.10->mujoco_py->d4rl) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio>=2.1.2->mujoco_py->d4rl) (8.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->gym<0.24.0->d4rl) (3.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->dm_control>=1.0.3->d4rl) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->dm_control>=1.0.3->d4rl) (1.26.15)\n",
            "Building wheels for collected packages: d4rl, gym, mjrl\n",
            "  Building wheel for d4rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d4rl: filename=d4rl-1.1-py3-none-any.whl size=26412096 sha256=77146f2d22d14241c5772957e68a80868bb29d5d18f2a52f6eaeca75bb43b3d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j90bl8wa/wheels/41/02/37/f4451771e38edd61c258167d1133c069dd7a2564b7752e2cc9\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701375 sha256=25c975ce1f4cb0b70eec9ba87dcebc953286534f1e7d543ed104b7ea34756864\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/be/7e/92a54668db96883e38ce60a9249dc55de7cd6eee49e7311940\n",
            "  Building wheel for mjrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mjrl: filename=mjrl-1.0.0-py3-none-any.whl size=61954 sha256=83f8de7996b482a742ad8ec0bddf14506f4675c8cd4906f05fca0cbd9ccf0e52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j90bl8wa/wheels/f4/3e/63/a8fcfedc1d6b33f99f6e2a29e97bdef827b5471c4f34c0ded0\n",
            "Successfully built d4rl gym mjrl\n",
            "Installing collected packages: pybullet, mjrl, mujoco, labmaze, dm-env, gym, dm_control, d4rl\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed d4rl-1.1 dm-env-1.6 dm_control-1.0.11 gym-0.23.1 labmaze-1.0.6 mjrl-1.0.0 mujoco-2.3.3 pybullet-3.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN-aQV6vwDSF",
        "outputId": "de89e789-155d-4c44-936e-73dc3b816198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=7e14dbdb801d0b4556ae24cce4cc5b01af8f507a8b09fd8fe27628afa3b45483\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEzeXtx8dpCF",
        "outputId": "4694ea21-85fd-4a55-c468-054d75015c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
            "No module named 'flow'\n",
            "/usr/local/lib/python3.9/dist-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
            "  warnings.warn(message, GLFWError)\n",
            "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
            "No module named 'carla'\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import torch\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from copy import deepcopy\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym\n",
        "import d4rl\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2Z97tLJdpCF"
      },
      "outputs": [],
      "source": [
        "max_target_returns = {\n",
        "    \"halfcheetah-medium-replay-v0\": 15.743,\n",
        "    \"halfcheetah-medium-v0\": 15.743,\n",
        "    \"hopper-medium-replay-v0\": 6.918,\n",
        "    \"hopper-medium-v0\": 6.918,\n",
        "    \"walker2d-medium-replay-v0\": 10.271,\n",
        "    \"walker2d-medium-v0\": 10.271\n",
        "}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class train_config:\n",
        "    policy: str = \"REDQ_BC\"\n",
        "    env: str = \"walker2d-medium-v0\" # [halfcheetah-medium-replay-v0 walker2d-medium-replay-v0]\n",
        "    seed: int = 42\n",
        "    eval_frequency: int = 5000\n",
        "    max_timesteps: int = 250000\n",
        "    pretrain_timesteps: int = 1000000\n",
        "    num_updates: int = 10\n",
        "    save_model: bool = True\n",
        "    load_policy_path: str = \"\"\n",
        "    episode_length: int = 1000\n",
        "    exploration_noise: float = 0.1  # standard deviation of a gaussian devoted to the action space exploration noise\n",
        "    batch_size: int = 256\n",
        "    discount_factor: float = 0.99\n",
        "    tau: float = 0.005  # see algo.jpeg in 'paper' folder\n",
        "    policy_noise: float = 0.2\n",
        "    noise_clip: float = 0.5\n",
        "    policy_frequency: int = 2\n",
        "    alpha: float = 0.4\n",
        "    alpha_finetune: float = 0.4\n",
        "    sample_method: str = \"random\"  # best\n",
        "    sample_ratio: float = 0.05  # see algo.jpeg in 'paper' folder (ratio to keep offline data in replay buffer)\n",
        "    minimize_over_q: bool = False  # if false, use randomized ensembles, else min Q values for steps, see eq3.PNG in 'paper' folder\n",
        "    Kp: float = 0.00003 # see eq2.PNG in 'paper' folder\n",
        "    Kd: float = 0.0001 # see eq2.PNG in 'paper' folder\n",
        "    normalize_returns: bool = True  # if true, divide returns by a factor of a target return defined in 'max_target_returns' dataclass\n",
        "\n",
        "cfg = train_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZWVDGjHdpCG"
      },
      "outputs": [],
      "source": [
        "run_name = f\"redq_bc_weights/redq_bc_{cfg.env}_{cfg.seed}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvrP91XJdpCG"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 max_action: float,\n",
        "                 hidden_dim: int = 256) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        return self.max_action * self.actor(state)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def act(self, state, device: str = \"cpu\") -> np.ndarray:\n",
        "        state = state.reshape(1, -1)\n",
        "\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "        \n",
        "        return self(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "class EnsembleLinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 ensemble_size: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.ensemble_size = ensemble_size\n",
        "        scale_factor = 2 * in_features ** 0.5\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(ensemble_size, in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(ensemble_size, 1, out_features))\n",
        "\n",
        "        nn.init.trunc_normal_(self.weight, std=1 / scale_factor)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        if len(x.shape) == 2:\n",
        "            #print(x.shape, self.weight.shape)\n",
        "            x = torch.einsum('ij,bjk->bik', x, self.weight)\n",
        "        else:\n",
        "            x = torch.einsum('bij,bjk->bik', x, self.weight)\n",
        "        \n",
        "        x = x + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class EnsembledCritic(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 hidden_dim: int = 256,\n",
        "                 num_critics: int = 10) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.critics = nn.Sequential(\n",
        "            EnsembleLinear(state_dim + action_dim, hidden_dim, ensemble_size=num_critics),\n",
        "            nn.ReLU(),\n",
        "            EnsembleLinear(hidden_dim, hidden_dim, ensemble_size=num_critics),\n",
        "            nn.ReLU(),\n",
        "            EnsembleLinear(hidden_dim, 1, ensemble_size=num_critics)\n",
        "        )\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        # shape: (num_critics, batch, 1)\n",
        "        concat = torch.cat([state, action], 1)\n",
        "        #print(f\"concat shape {concat.shape}\")\n",
        "\n",
        "        #print(self.critics(concat).shape)\n",
        "        return self.critics(concat)\n",
        "\n",
        "\n",
        "class RandomizedEnsembles_BC:\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 max_action: float,\n",
        "                 discount_factor: float = 0.99,\n",
        "                 tau: float = 0.005,\n",
        "                 exploration_noise: float = 0.2,\n",
        "                 noise_clip: float = 0.5,\n",
        "                 policy_frequency: int = 2,\n",
        "                 num_q_networks: int = 10,\n",
        "                 alpha_finetune: float = 0.4,\n",
        "                 pretrain: bool = False,\n",
        "                 minimize_over_q: bool = False,\n",
        "                 Kp: float = 0.00003,\n",
        "                 Kd: float = 0.0001) -> None:\n",
        "        \n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = EnsembledCritic(state_dim, action_dim, num_critics=num_q_networks).to(device)\n",
        "        self.critic_target = deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount_factor = discount_factor\n",
        "        self.tau = tau\n",
        "        self.exploration_noise = exploration_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_frequency\n",
        "        self.num_nets = num_q_networks\n",
        "        self.alpha = alpha_finetune\n",
        "        self.alpha_finetune = alpha_finetune\n",
        "        self.pretrain = pretrain\n",
        "        self.minimize_over_q = minimize_over_q\n",
        "        self.kp = Kp\n",
        "        self.kd = Kd\n",
        "    \n",
        "    def update_alpha(self,\n",
        "                     episode_timesteps,\n",
        "                     average_return,\n",
        "                     current_return,\n",
        "                     target_return: float = 1.05) -> None:\n",
        "        # see eq2.PNG in 'paper' folder\n",
        "        self.alpha += episode_timesteps * (self.kp * (average_return - target_return) + self.kd * max(0, average_return - current_return))\n",
        "        self.alpha = max(0.0, min(self.alpha, self.alpha_finetune))\n",
        "    \n",
        "    def train(self, data):\n",
        "        self.iteration = 1\n",
        "\n",
        "        state, action, reward, next_state, done = data\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.exploration_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "            \n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            if self.minimize_over_q and not self.pretrain:\n",
        "                print(f\"input shape: {next_state.shape}, {next_action.shape}\")\n",
        "                tgt_qs = self.critic_target(next_state, next_action)\n",
        "                tgt_q, _ = torch.min(tgt_qs, dim=0)\n",
        "            else:  # REDQ\n",
        "                random_indexes = np.random.permutation(self.num_nets)\n",
        "                tgt_qs = self.critic_target(next_state, next_action)[random_indexes]\n",
        "                tgt_q1, tgt_q2 = tgt_qs[:2]\n",
        "                tgt_q = torch.min(tgt_q1, tgt_q2)\n",
        "            \n",
        "            tgt_q = reward + (1 - done) * self.discount_factor * tgt_q\n",
        "        \n",
        "        current_qs = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(current_qs.unsqueeze(0), tgt_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # policy update\n",
        "        if not self.iteration % self.policy_freq:\n",
        "\n",
        "            pi = self.actor(state)\n",
        "            q = self.critic(state, pi).mean(0)\n",
        "            \n",
        "            actor_loss = -q.mean() / q.abs().mean().detach() + self.alpha * F.mse_loss(pi, action)\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            self.soft_update(\"actor\")\n",
        "            self.soft_update(\"critic\")\n",
        "\n",
        "        return {\n",
        "            \"critic_loss\": critic_loss.item(),\n",
        "            \"critic_Qs\": current_qs[0].mean().item()}\n",
        "\n",
        "    \n",
        "    def act(self, state):\n",
        "        if len(state.shape) == 1:\n",
        "            state = state.reshape(1, -1)\n",
        "        \n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = self.to_tensor(state, device=self.device)\n",
        "        else:\n",
        "            state = state.to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state)\n",
        "        \n",
        "        return action.cpu().data.numpy().flatten()\n",
        "    \n",
        "    def soft_update(self, regime):\n",
        "        if regime == \"actor\":\n",
        "            for param, tgt_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
        "        else:\n",
        "            for param, tgt_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def to_tensor(data, device=None) -> torch.Tensor:\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    \n",
        "    def save(self, filename):\n",
        "        torch.save({\n",
        "            \"critic\": self.critic.state_dict(),\n",
        "            \"critic_optimizer\": self.critic_optimizer.state_dict(),\n",
        "            \"actor\": self.actor.state_dict(),\n",
        "            \"actor_optimizer\": self.actor_optimizer.state_dict()\n",
        "        }, filename + '_policy.pth')\n",
        "\n",
        "    def load(self, filename):\n",
        "        policy_dict = torch.load(filename + \"_policy.pth\")\n",
        "\n",
        "        self.critic.load_state_dict(policy_dict[\"critic\"])\n",
        "        self.critic_optimizer.load_state_dict(policy_dict[\"critic_optimizer\"])\n",
        "        self.critic_target = deepcopy(self.critic)\n",
        "\n",
        "        self.actor.load_state_dict(policy_dict[\"actor\"])\n",
        "        self.actor_optimizer.load_state_dict(policy_dict[\"actor_optimizer\"])\n",
        "        self.actor_target = deepcopy(self.actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPYtdpaBdpCH"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    data_size_threshold = 50000\n",
        "    distill_methods = [\"random\", \"best\"]\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 buffer_size: int = 1000000) -> None:\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.buffer_size = buffer_size\n",
        "        self.pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        self.states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
        "        self.actions = torch.zeros((buffer_size, action_dim), dtype=torch.float32, device=device)\n",
        "        self.rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
        "        self.next_states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
        "        self.dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "        # i/o order: state, action, reward, next_state, done\n",
        "    \n",
        "    @staticmethod\n",
        "    def to_tensor(data: np.ndarray, device=None) -> torch.Tensor:\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    \n",
        "    def from_json(self, json_file):\n",
        "        import json\n",
        "        json_file = os.path.join(\"json_datasets\", json_file)\n",
        "        output = dict()\n",
        "\n",
        "        with open(json_file) as f:\n",
        "            dataset = json.load(f)\n",
        "        \n",
        "        for k, v in dataset.items():\n",
        "            v = np.array(v)\n",
        "            if k != \"terminals\":\n",
        "                v = v.astype(np.float32)\n",
        "            \n",
        "            output[k] = v\n",
        "        \n",
        "        self.from_d4rl(output)\n",
        "    \n",
        "    def sample(self, batch_size: int):\n",
        "        indexes = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            self.to_tensor(self.states[indexes], self.device),\n",
        "            self.to_tensor(self.actions[indexes], self.device),\n",
        "            self.to_tensor(self.rewards[indexes], self.device),\n",
        "            self.to_tensor(self.next_states[indexes], self.device),\n",
        "            self.to_tensor(self.dones[indexes], self.device)\n",
        "        )\n",
        "    \n",
        "    def from_d4rl(self, dataset):\n",
        "        if self.size:\n",
        "            print(\"Warning: loading data into non-empty buffer\")\n",
        "        n_transitions = dataset[\"observations\"].shape[0]\n",
        "\n",
        "        if n_transitions < self.buffer_size:\n",
        "            self.states[:n_transitions] = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
        "            self.actions[:n_transitions] = self.to_tensor(dataset[\"actions\"][-n_transitions:], self.device)\n",
        "            self.next_states[:n_transitions] = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
        "            self.rewards[:n_transitions] = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "            self.dones[:n_transitions] = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "\n",
        "        else:\n",
        "            self.buffer_size = n_transitions\n",
        "\n",
        "            self.states = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
        "            self.actions = self.to_tensor(dataset[\"actions\"][-n_transitions:])\n",
        "            self.next_states = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
        "            self.rewards = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "            self.dones = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
        "        \n",
        "        self.size = n_transitions\n",
        "        self.pointer = n_transitions % self.buffer_size\n",
        "    \n",
        "    def normalize_states(self, eps=1e-3):\n",
        "        mean = self.states.mean(0, keepdim=True)\n",
        "        std = self.states.std(0, keepdim=True) + eps\n",
        "        self.states = (self.states - mean) / std\n",
        "        self.next_states = (self.next_states - mean) / std\n",
        "        return mean, std\n",
        "    \n",
        "    def get_all(self):\n",
        "        return (\n",
        "            self.states[:self.size].to(self.device),\n",
        "            self.actions[:self.size].to(self.device),\n",
        "            self.rewards[:self.size].to(self.device),\n",
        "            self.next_states[:self.size].to(self.device),\n",
        "            self.dones[:self.size].to(self.device)\n",
        "        )\n",
        "\n",
        "    def add_transition(self,\n",
        "                       state: torch.Tensor,\n",
        "                       action: torch.Tensor,\n",
        "                       reward: torch.Tensor,\n",
        "                       next_state: torch.Tensor,\n",
        "                       done: torch.Tensor):\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = self.to_tensor(state)\n",
        "            action = self.to_tensor(action)\n",
        "            reward = self.to_tensor(reward)\n",
        "            next_state = self.to_tensor(next_state)\n",
        "            done = self.to_tensor(done)\n",
        "\n",
        "\n",
        "        self.states[self.pointer] = state\n",
        "        self.actions[self.pointer] = action\n",
        "        self.rewards[self.pointer] = reward\n",
        "        self.next_states[self.pointer] = next_state\n",
        "        self.dones[self.pointer] = done\n",
        "\n",
        "        self.pointer = (self.pointer + 1) % self.buffer_size\n",
        "        self.size = min(self.size + 1, self.buffer_size)\n",
        "    \n",
        "    def add_batch(self,\n",
        "                  states: List[torch.Tensor],\n",
        "                  actions: List[torch.Tensor],\n",
        "                  rewards: List[torch.Tensor],\n",
        "                  next_states: List[torch.Tensor],\n",
        "                  dones: List[torch.Tensor]):\n",
        "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
        "            self.add_transition(state, action, reward, next_state, done)\n",
        "    \n",
        "    def distill(self,\n",
        "                dataset,\n",
        "                env_name,\n",
        "                sample_method,\n",
        "                ratio=0.05):\n",
        "        data_size = max(int(ratio * dataset[\"observations\"].shape[0]), self.data_size_threshold)\n",
        "        assert sample_method in self.distill_methods, \"Unknown sample method\"\n",
        "\n",
        "        if sample_method == \"random\":\n",
        "            indexes = np.random.randint(0, dataset[\"observations\"].shape[0], size=data_size)\n",
        "        if sample_method == \"best\":\n",
        "            full_datas_size = dataset[\"observations\"].shape[0]\n",
        "            indexes = np.arange(full_datas_size - data_size)\n",
        "        \n",
        "        if data_size < self.buffer_size:\n",
        "            self.states[:data_size] = self.to_tensor(dataset[\"observations\"][indexes], self.device)\n",
        "            self.actions[:data_size] = self.to_tensor(dataset[\"actions\"][indexes], self.device)\n",
        "            self.rewards[:data_size] = self.to_tensor(dataset[\"rewards\"][indexes].reshape(-1, 1), self.device)\n",
        "            self.next_states[:data_size] = self.to_tensor(dataset[\"next_observations\"][indexes], self.device)\n",
        "            self.dones[:data_size] = self.to_tensor(dataset[\"terminals\"][indexes].reshape(-1, 1), self.device)\n",
        "        else:\n",
        "            self.buffer_size = data_size\n",
        "            self.states = self.to_tensor(dataset[\"observations\"][indexes], self.device)\n",
        "            self.actions = self.to_tensor(dataset[\"actions\"][indexes], self.device)\n",
        "            self.rewards = self.to_tensor(dataset[\"rewards\"][indexes].reshape(-1, 1), self.device)\n",
        "            self.next_states = self.to_tensor(dataset[\"next_observations\"][indexes], self.device)\n",
        "            self.dones = self.to_tensor(dataset[\"terminals\"][indexes].reshape(-1, 1), self.device)\n",
        "        \n",
        "        self.size = data_size\n",
        "        self.pointer = data_size % self.buffer_size\n",
        "    \n",
        "    @staticmethod\n",
        "    def dataset_stats(dataset):\n",
        "        episode_returns = []\n",
        "        returns = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        for reward, done in zip(dataset[\"rewards\"], dataset[\"terminals\"]):\n",
        "            if done:\n",
        "                episode_returns.append(returns)\n",
        "                returns = 0\n",
        "                episode_length = 0\n",
        "            else:\n",
        "                episode_length += 1\n",
        "                returns += reward\n",
        "                if episode_length == 1000:\n",
        "                    episode_returns.append(returns)\n",
        "                    returns = 0\n",
        "                    episode_length = 0\n",
        "\n",
        "        episode_returns = np.array(episode_returns)\n",
        "        return episode_returns.mean(), episode_returns.std()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_arguments = {\n",
        "    \"hopper-medium-replay-v0\": [11, 3, 1.0],\n",
        "    \"hopper-medium-v0\": [11, 3, 1.0],\n",
        "    \"walker2d-medium-replay-v0\": [17, 6, 1.0],\n",
        "    \"walker2d-medium-v0\": [17, 6, 1.0],\n",
        "    \"halfcheetah-medium-replay-v0\": [17, 6, 1.0],\n",
        "    \"halfcheetah-medium-v0\": [17, 6, 1.0]\n",
        "}"
      ],
      "metadata": {
        "id": "C8VNsHCZwUoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTvbHKoNdpCH"
      },
      "outputs": [],
      "source": [
        "policy = RandomizedEnsembles_BC(*init_arguments[cfg.env])\n",
        "policy.load(run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFDGey_ZdpCI",
        "outputId": "8d389dfa-72c7-4014-f1ce-6cb948e5a10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment walker2d-medium-v0 is out of date. You should consider upgrading to version `v2` with the environment ID `walker2d-medium-v2`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/d4rl/gym_mujoco/gym_envs.py:23: UserWarning: \u001b[33mThis environment is deprecated. Please use the most recent version of this environment.\u001b[0m\n",
            "  offline_env.OfflineEnv.__init__(self, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco/walker2d_medium.hdf5 to /root/.d4rl/datasets/walker2d_medium.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 5/5 [00:00<00:00,  7.19it/s]\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(cfg.env)\n",
        "random.seed(cfg.seed)\n",
        "np.random.seed(cfg.seed)\n",
        "torch.manual_seed(cfg.seed)\n",
        "\n",
        "env.seed(cfg.seed)\n",
        "env.action_space.seed(cfg.seed)\n",
        "env.observation_space.seed(cfg.seed)\n",
        "\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0] \n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "state, done = env.reset(), False\n",
        "episode_timesteps = 0\n",
        "update_info, eval_info = {}, {}\n",
        "\n",
        "buffer = ReplayBuffer(state_dim, action_dim, buffer_size=cfg.max_timesteps)\n",
        "buffer.distill(d4rl.qlearning_dataset(env), cfg.env, cfg.sample_method, cfg.sample_ratio)\n",
        "\n",
        "policy.alpha = cfg.alpha_finetune\n",
        "policy.pretrain = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn4xhkz3dpCI"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy: RandomizedEnsembles_BC,\n",
        "                    env_name: str,\n",
        "                    seed=cfg.seed,\n",
        "                    eval_episodes=10):\n",
        "    \n",
        "    eval_env = gym.make(env_name)\n",
        "    eval_env.seed(seed + 42)\n",
        "\n",
        "    average_reward, average_length = 0, 0\n",
        "\n",
        "    for _ in range(eval_episodes):\n",
        "        state, done = eval_env.reset(), False\n",
        "\n",
        "        while not done:\n",
        "            action = policy.act(state)\n",
        "            state, reward, done, _ = eval_env.step(action)\n",
        "            average_reward += reward\n",
        "            average_length += 1\n",
        "        \n",
        "        average_reward /= eval_episodes\n",
        "        average_length = int(average_length / eval_episodes)\n",
        "\n",
        "        d4rl_score = eval_env.get_normalized_score(average_reward) * 100\n",
        "\n",
        "        return {\n",
        "            \"d4rl\": d4rl_score,\n",
        "            \"evaluation\": average_reward,\n",
        "            \"length\": average_length\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "_bXofqdPdpCI",
        "outputId": "7e1e45f1-1bd2-46c9-ab2d-75e0719ca07f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230408_172228-9qchw88m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zzmtsvv/adaptive_bc/runs/9qchw88m' target=\"_blank\">redq_bc_weights/redq_bc_walker2d-medium-v0_42</a></strong> to <a href='https://wandb.ai/zzmtsvv/adaptive_bc' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/zzmtsvv/adaptive_bc' target=\"_blank\">https://wandb.ai/zzmtsvv/adaptive_bc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/zzmtsvv/adaptive_bc/runs/9qchw88m' target=\"_blank\">https://wandb.ai/zzmtsvv/adaptive_bc/runs/9qchw88m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/250000 [00:00<?, ?it/s]<ipython-input-17-7bc9cc4c3745>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(data, dtype=torch.float32, device=device)\n",
            "<ipython-input-10-f0f028f87d65>:159: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([1, 10, 256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  critic_loss = F.mse_loss(current_qs.unsqueeze(0), tgt_q)\n",
            " 24%|██▎       | 59336/250000 [40:01<1:54:36, 27.73it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "max_return = max_target_returns[cfg.env]\n",
        "with wandb.init(project='adaptive_bc', group=cfg.env, job_type=\"finetune\", name=run_name):\n",
        "    wandb.config.update({k: v for k, v in cfg.__dict__.items() if not k.startswith('__')})\n",
        "\n",
        "    episode_return = 0.0\n",
        "    if cfg.normalize_returns:\n",
        "        last_return = evaluate_policy(policy, cfg.env)[\"evaluation\"] / max_return\n",
        "    else:\n",
        "        last_return = evaluate_policy(policy, cfg.env)[\"d4rl\"] * 0.01\n",
        "        \n",
        "    current_return = last_return\n",
        "    target_return = 1.05\n",
        "\n",
        "    for timestep in tqdm(range(cfg.max_timesteps)):\n",
        "        episode_timesteps += 1\n",
        "        \n",
        "        action = (policy.act(state) + np.random.normal(0, scale=cfg.exploration_noise, size=action_dim)).clip(-max_action, max_action)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_return += reward\n",
        "\n",
        "        done = float(done) if episode_timesteps < env._max_episode_steps else 0.0\n",
        "        buffer.add_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        for _ in range(cfg.num_updates):\n",
        "            update_info = policy.train(buffer.sample(cfg.batch_size))\n",
        "        \n",
        "        update_info.update({'current_return': current_return, 'last_return': last_return})\n",
        "\n",
        "        wandb.log({'online_training/': update_info,\n",
        "                   'online_trainig/alpha': policy.alpha})\n",
        "        \n",
        "        if done:\n",
        "            state, done = env.reset(), False\n",
        "\n",
        "            if cfg.normalize_returns:\n",
        "                current_return = episode_return / max_return\n",
        "            else:\n",
        "                current_return = env.get_normalized_score(episode_return)\n",
        "            \n",
        "            policy.update_alpha(episode_timesteps, last_return, current_return)\n",
        "\n",
        "            episode_timesteps = 0\n",
        "            episode_return = 0\n",
        "        \n",
        "        if not timestep % cfg.eval_frequency:\n",
        "            eval_info = evaluate_policy(policy, cfg.env, cfg.seed)\n",
        "            wandb.log({'online_evaluation/': eval_info})\n",
        "\n",
        "            if cfg.save_model:\n",
        "                policy.save(f\"online_{run_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/wandb.zip /content/wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osj8r8gTcn9h",
        "outputId": "cbbf22b8-e8b6-48d2-9346-1262678fa91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/wandb/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/wandb-metadata.json (deflated 57%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/config.yaml (deflated 66%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/output.log (deflated 86%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/files/wandb-summary.json (deflated 39%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/logs/debug.log (deflated 71%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/logs/debug-internal.log (deflated 96%)\n",
            "  adding: content/wandb/run-20230408_134723-vv2aksev/run-vv2aksev.wandb (deflated 81%)\n",
            "  adding: content/wandb/debug.log (deflated 71%)\n",
            "  adding: content/wandb/latest-run/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/tmp/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/files/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/files/wandb-metadata.json (deflated 57%)\n",
            "  adding: content/wandb/latest-run/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/latest-run/files/config.yaml (deflated 66%)\n",
            "  adding: content/wandb/latest-run/files/output.log (deflated 86%)\n",
            "  adding: content/wandb/latest-run/files/wandb-summary.json (deflated 39%)\n",
            "  adding: content/wandb/latest-run/logs/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/logs/debug.log (deflated 71%)\n",
            "  adding: content/wandb/latest-run/logs/debug-internal.log (deflated 96%)\n",
            "  adding: content/wandb/latest-run/run-vv2aksev.wandb (deflated 81%)\n",
            "  adding: content/wandb/debug-internal.log (deflated 96%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/wandb-metadata.json (deflated 57%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/config.yaml (deflated 66%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/output.log (deflated 86%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/files/wandb-summary.json (deflated 39%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/run-y599p8w0.wandb (deflated 81%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/logs/debug.log (deflated 71%)\n",
            "  adding: content/wandb/run-20230408_121834-y599p8w0/logs/debug-internal.log (deflated 96%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/run-7xgu4vuw.wandb (deflated 51%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/wandb-metadata.json (deflated 57%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/config.yaml (deflated 66%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/output.log (deflated 42%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/files/wandb-summary.json (deflated 41%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/logs/debug.log (deflated 71%)\n",
            "  adding: content/wandb/run-20230408_121702-7xgu4vuw/logs/debug-internal.log (deflated 87%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vnpL0dkcqM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}