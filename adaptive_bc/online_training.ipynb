{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include this at the top of your colab code\n",
    "import os\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "  # Get the prereqs\n",
    "  !apt-get -qq update\n",
    "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "  # Get Mujoco\n",
    "  !mkdir ~/.mujoco\n",
    "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "  !rm mujoco.tar.gz\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "  !ldconfig\n",
    "  # Install Mujoco-py\n",
    "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "  # run once\n",
    "  !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "  if _mujoco_run_once:\n",
    "    pass\n",
    "except NameError:\n",
    "  _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/usr/lib/nvidia'\n",
    "  except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "  try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  # presetup so we don't see output on first env initialization\n",
    "  import mujoco_py\n",
    "  _mujoco_run_once = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import d4rl\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_returns = {\n",
    "    \"halfcheetah-medium-replay-v0\": 15.743,\n",
    "    \"halfcheetah-medium-v0\": 15.743,\n",
    "    \"hopper-medium-replay-v0\": 6.918,\n",
    "    \"hopper-medium-v0\": 6.918,\n",
    "    \"walker2d-medium-replay-v0\": 10.271,\n",
    "    \"walker2d-medium-v0\": 10.271\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class train_config:\n",
    "    policy: str = \"REDQ_BC\"\n",
    "    env: str = \"hopper-medium-replay-v0\" # [halfcheetah-medium-replay-v0 walker2d-medium-replay-v0]\n",
    "    seed: int = 42\n",
    "    eval_frequency: int = 5000\n",
    "    max_timesteps: int = 250000\n",
    "    pretrain_timesteps: int = 1000000\n",
    "    num_updates: int = 10\n",
    "    save_model: bool = True\n",
    "    load_policy_path: str = \"\"\n",
    "    episode_length: int = 1000\n",
    "    exploration_noise: float = 0.1  # standard deviation of a gaussian devoted to the action space exploration noise\n",
    "    batch_size: int = 256\n",
    "    discount_factor: float = 0.99\n",
    "    tau: float = 0.005  # see algo.jpeg in 'paper' folder\n",
    "    policy_noise: float = 0.2\n",
    "    noise_clip: float = 0.5\n",
    "    policy_frequency: int = 2\n",
    "    alpha: float = 0.4\n",
    "    alpha_finetune: float = 0.4\n",
    "    sample_method: str = \"random\"  # best\n",
    "    sample_ratio: float = 0.05  # see algo.jpeg in 'paper' folder (ratio to keep offline data in replay buffer)\n",
    "    minimize_over_q: bool = False  # if false, use randomized ensembles, else min Q values for steps, see eq3.PNG in 'paper' folder\n",
    "    Kp: float = 0.00003 # see eq2.PNG in 'paper' folder\n",
    "    Kd: float = 0.0001 # see eq2.PNG in 'paper' folder\n",
    "    normalize_returns: bool = True  # if true, divide returns by a factor of a target return defined in 'max_target_returns' dataclass\n",
    "\n",
    "cfg = train_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"redq_bc_{cfg.env}_{cfg.seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 max_action: float,\n",
    "                 hidden_dim: int = 256) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.max_action * self.actor(state)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, state, device: str = \"cpu\") -> np.ndarray:\n",
    "        state = state.reshape(1, -1)\n",
    "\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return self(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "class EnsembleLinear(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 ensemble_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        scale_factor = 2 * in_features ** 0.5\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(ensemble_size, in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(ensemble_size, 1, out_features))\n",
    "\n",
    "        nn.init.trunc_normal_(self.weight, std=1 / scale_factor)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            #print(x.shape, self.weight.shape)\n",
    "            x = torch.einsum('ij,bjk->bik', x, self.weight)\n",
    "        else:\n",
    "            x = torch.einsum('bij,bjk->bik', x, self.weight)\n",
    "        \n",
    "        x = x + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class EnsembledCritic(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 hidden_dim: int = 256,\n",
    "                 num_critics: int = 10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.critics = nn.Sequential(\n",
    "            EnsembleLinear(state_dim + action_dim, hidden_dim, ensemble_size=num_critics),\n",
    "            nn.ReLU(),\n",
    "            EnsembleLinear(hidden_dim, hidden_dim, ensemble_size=num_critics),\n",
    "            nn.ReLU(),\n",
    "            EnsembleLinear(hidden_dim, 1, ensemble_size=num_critics)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        # shape: (num_critics, batch, 1)\n",
    "        concat = torch.cat([state, action], 1)\n",
    "        #print(f\"concat shape {concat.shape}\")\n",
    "\n",
    "        #print(self.critics(concat).shape)\n",
    "        return self.critics(concat)\n",
    "\n",
    "\n",
    "class RandomizedEnsembles_BC:\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 max_action: float,\n",
    "                 discount_factor: float = 0.99,\n",
    "                 tau: float = 0.005,\n",
    "                 exploration_noise: float = 0.2,\n",
    "                 noise_clip: float = 0.5,\n",
    "                 policy_frequency: int = 2,\n",
    "                 num_q_networks: int = 10,\n",
    "                 alpha_finetune: float = 0.4,\n",
    "                 pretrain: bool = False,\n",
    "                 minimize_over_q: bool = False,\n",
    "                 Kp: float = 0.00003,\n",
    "                 Kd: float = 0.0001) -> None:\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = EnsembledCritic(state_dim, action_dim, num_critics=num_q_networks).to(device)\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_frequency\n",
    "        self.num_nets = num_q_networks\n",
    "        self.alpha = alpha_finetune\n",
    "        self.alpha_finetune = alpha_finetune\n",
    "        self.pretrain = pretrain\n",
    "        self.minimize_over_q = minimize_over_q\n",
    "        self.kp = Kp\n",
    "        self.kd = Kd\n",
    "    \n",
    "    def update_alpha(self,\n",
    "                     episode_timesteps,\n",
    "                     average_return,\n",
    "                     current_return,\n",
    "                     target_return: float = 1.05) -> None:\n",
    "        # see eq2.PNG in 'paper' folder\n",
    "        self.alpha += episode_timesteps * (self.kp * (average_return - target_return) + self.kd * max(0, average_return - current_return))\n",
    "        self.alpha = max(0.0, min(self.alpha, self.alpha_finetune))\n",
    "    \n",
    "    def train(self, data):\n",
    "        self.iteration = 1\n",
    "\n",
    "        state, action, reward, next_state, done = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * self.exploration_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            if self.minimize_over_q and not self.pretrain:\n",
    "                print(f\"input shape: {next_state.shape}, {next_action.shape}\")\n",
    "                tgt_qs = self.critic_target(next_state, next_action)\n",
    "                tgt_q, _ = torch.min(tgt_qs, dim=0)\n",
    "            else:  # REDQ\n",
    "                random_indexes = np.random.permutation(self.num_nets)\n",
    "                tgt_qs = self.critic_target(next_state, next_action)[random_indexes]\n",
    "                tgt_q1, tgt_q2 = tgt_qs[:2]\n",
    "                tgt_q = torch.min(tgt_q1, tgt_q2)\n",
    "            \n",
    "            tgt_q = reward + (1 - done) * self.discount_factor * tgt_q\n",
    "        \n",
    "        current_qs = self.critic(state, action)\n",
    "\n",
    "        critic_loss = F.mse_loss(current_qs.unsqueeze(0), tgt_q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # policy update\n",
    "        if not self.iteration % self.policy_freq:\n",
    "\n",
    "            pi = self.actor(state)\n",
    "            q = self.critic(state, pi).mean(0)\n",
    "            \n",
    "            actor_loss = -q.mean() / q.abs().mean().detach() + self.alpha * F.mse_loss(pi, action)\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            self.soft_update(\"actor\")\n",
    "            self.soft_update(\"critic\")\n",
    "\n",
    "        return {\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "            \"critic_Qs\": current_qs[0].mean().item()}\n",
    "\n",
    "    \n",
    "    def act(self, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(1, -1)\n",
    "        \n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = self.to_tensor(state, device=self.device)\n",
    "        else:\n",
    "            state = state.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        \n",
    "        return action.cpu().data.numpy().flatten()\n",
    "    \n",
    "    def soft_update(self, regime):\n",
    "        if regime == \"actor\":\n",
    "            for param, tgt_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
    "        else:\n",
    "            for param, tgt_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                tgt_param.data.copy_(self.tau * param.data + (1 - self.tau) * tgt_param.data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_tensor(data, device=None) -> torch.Tensor:\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            \"critic\": self.critic.state_dict(),\n",
    "            \"critic_optimizer\": self.critic_optimizer.state_dict(),\n",
    "            \"actor\": self.actor.state_dict(),\n",
    "            \"actor_optimizer\": self.actor_optimizer.state_dict()\n",
    "        }, filename + '_policy.pth')\n",
    "\n",
    "    def load(self, filename):\n",
    "        policy_dict = torch.load(filename + \"_policy.pth\")\n",
    "\n",
    "        self.critic.load_state_dict(policy_dict[\"critic\"])\n",
    "        self.critic_optimizer.load_state_dict(policy_dict[\"critic_optimizer\"])\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(policy_dict[\"actor\"])\n",
    "        self.actor_optimizer.load_state_dict(policy_dict[\"actor_optimizer\"])\n",
    "        self.actor_target = deepcopy(self.actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    data_size_threshold = 50000\n",
    "    distill_methods = [\"random\", \"best\"]\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 buffer_size: int = 1000000) -> None:\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pointer = 0\n",
    "        self.size = 0\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        self.states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
    "        self.actions = torch.zeros((buffer_size, action_dim), dtype=torch.float32, device=device)\n",
    "        self.rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self.next_states = torch.zeros((buffer_size, state_dim), dtype=torch.float32, device=device)\n",
    "        self.dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "\n",
    "        # i/o order: state, action, reward, next_state, done\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_tensor(data: np.ndarray, device=None) -> torch.Tensor:\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        return torch.tensor(data, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def from_json(self, json_file):\n",
    "        import json\n",
    "        json_file = os.path.join(\"json_datasets\", json_file)\n",
    "        output = dict()\n",
    "\n",
    "        with open(json_file) as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        for k, v in dataset.items():\n",
    "            v = np.array(v)\n",
    "            if k != \"terminals\":\n",
    "                v = v.astype(np.float32)\n",
    "            \n",
    "            output[k] = v\n",
    "        \n",
    "        self.from_d4rl(output)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        indexes = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (\n",
    "            self.to_tensor(self.states[indexes], self.device),\n",
    "            self.to_tensor(self.actions[indexes], self.device),\n",
    "            self.to_tensor(self.rewards[indexes], self.device),\n",
    "            self.to_tensor(self.next_states[indexes], self.device),\n",
    "            self.to_tensor(self.dones[indexes], self.device)\n",
    "        )\n",
    "    \n",
    "    def from_d4rl(self, dataset):\n",
    "        if self.size:\n",
    "            print(\"Warning: loading data into non-empty buffer\")\n",
    "        n_transitions = dataset[\"observations\"].shape[0]\n",
    "\n",
    "        if n_transitions < self.buffer_size:\n",
    "            self.states[:n_transitions] = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
    "            self.actions[:n_transitions] = self.to_tensor(dataset[\"actions\"][-n_transitions:], self.device)\n",
    "            self.next_states[:n_transitions] = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
    "            self.rewards[:n_transitions] = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
    "            self.dones[:n_transitions] = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
    "\n",
    "        else:\n",
    "            self.buffer_size = n_transitions\n",
    "\n",
    "            self.states = self.to_tensor(dataset[\"observations\"][-n_transitions:], self.device)\n",
    "            self.actions = self.to_tensor(dataset[\"actions\"][-n_transitions:])\n",
    "            self.next_states = self.to_tensor(dataset[\"next_observations\"][-n_transitions:], self.device)\n",
    "            self.rewards = self.to_tensor(dataset[\"rewards\"][-n_transitions:].reshape(-1, 1), self.device)\n",
    "            self.dones = self.to_tensor(dataset[\"terminals\"][-n_transitions:].reshape(-1, 1), self.device)\n",
    "        \n",
    "        self.size = n_transitions\n",
    "        self.pointer = n_transitions % self.buffer_size\n",
    "    \n",
    "    def normalize_states(self, eps=1e-3):\n",
    "        mean = self.states.mean(0, keepdim=True)\n",
    "        std = self.states.std(0, keepdim=True) + eps\n",
    "        self.states = (self.states - mean) / std\n",
    "        self.next_states = (self.next_states - mean) / std\n",
    "        return mean, std\n",
    "    \n",
    "    def get_all(self):\n",
    "        return (\n",
    "            self.states[:self.size].to(self.device),\n",
    "            self.actions[:self.size].to(self.device),\n",
    "            self.rewards[:self.size].to(self.device),\n",
    "            self.next_states[:self.size].to(self.device),\n",
    "            self.dones[:self.size].to(self.device)\n",
    "        )\n",
    "\n",
    "    def add_transition(self,\n",
    "                       state: torch.Tensor,\n",
    "                       action: torch.Tensor,\n",
    "                       reward: torch.Tensor,\n",
    "                       next_state: torch.Tensor,\n",
    "                       done: torch.Tensor):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = self.to_tensor(state)\n",
    "            action = self.to_tensor(action)\n",
    "            reward = self.to_tensor(reward)\n",
    "            next_state = self.to_tensor(next_state)\n",
    "            done = self.to_tensor(done)\n",
    "\n",
    "\n",
    "        self.states[self.pointer] = state\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] = reward\n",
    "        self.next_states[self.pointer] = next_state\n",
    "        self.dones[self.pointer] = done\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.buffer_size\n",
    "        self.size = min(self.size + 1, self.buffer_size)\n",
    "    \n",
    "    def add_batch(self,\n",
    "                  states: List[torch.Tensor],\n",
    "                  actions: List[torch.Tensor],\n",
    "                  rewards: List[torch.Tensor],\n",
    "                  next_states: List[torch.Tensor],\n",
    "                  dones: List[torch.Tensor]):\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.add_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    def distill(self,\n",
    "                dataset,\n",
    "                env_name,\n",
    "                sample_method,\n",
    "                ratio=0.05):\n",
    "        data_size = max(int(ratio * dataset[\"observations\"].shape[0]), self.data_size_threshold)\n",
    "        assert sample_method in self.distill_methods, \"Unknown sample method\"\n",
    "\n",
    "        if sample_method == \"random\":\n",
    "            indexes = np.random.randint(0, dataset[\"observations\"].shape[0], size=data_size)\n",
    "        if sample_method == \"best\":\n",
    "            full_datas_size = dataset[\"observations\"].shape[0]\n",
    "            indexes = np.arange(full_datas_size - data_size)\n",
    "        \n",
    "        if data_size < self.buffer_size:\n",
    "            self.states[:data_size] = self.to_tensor(dataset[\"observations\"][indexes], self.device)\n",
    "            self.actions[:data_size] = self.to_tensor(dataset[\"actions\"][indexes], self.device)\n",
    "            self.rewards[:data_size] = self.to_tensor(dataset[\"rewards\"][indexes], self.device)\n",
    "            self.next_states[:data_size] = self.to_tensor(dataset[\"next_observations\"][indexes].reshape(-1, 1), self.device)\n",
    "            self.dones[:data_size] = self.to_tensor(dataset[\"terminals\"][indexes].reshape(-1, 1), self.device)\n",
    "        else:\n",
    "            self.buffer_size = data_size\n",
    "            self.states = self.to_tensor(dataset[\"observations\"][indexes], self.device)\n",
    "            self.actions = self.to_tensor(dataset[\"actions\"][indexes], self.device)\n",
    "            self.rewards = self.to_tensor(dataset[\"rewards\"][indexes], self.device)\n",
    "            self.next_states = self.to_tensor(dataset[\"next_observations\"][indexes].reshape(-1, 1), self.device)\n",
    "            self.dones = self.to_tensor(dataset[\"terminals\"][indexes].reshape(-1, 1), self.device)\n",
    "        \n",
    "        self.size = data_size\n",
    "        self.pointer = data_size % self.buffer_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def dataset_stats(dataset):\n",
    "        episode_returns = []\n",
    "        returns = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        for reward, done in zip(dataset[\"rewards\"], dataset[\"terminals\"]):\n",
    "            if done:\n",
    "                episode_returns.append(returns)\n",
    "                returns = 0\n",
    "                episode_length = 0\n",
    "            else:\n",
    "                episode_length += 1\n",
    "                returns += reward\n",
    "                if episode_length == 1000:\n",
    "                    episode_returns.append(returns)\n",
    "                    returns = 0\n",
    "                    episode_length = 0\n",
    "\n",
    "        episode_returns = np.array(episode_returns)\n",
    "        return episode_returns.mean(), episode_returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomizedEnsembles_BC()\n",
    "policy.load(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(cfg.env)\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "env.seed(cfg.seed)\n",
    "env.action_space.seed(cfg.seed)\n",
    "env.observation_space.seed(cfg.seed)\n",
    "\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_timesteps = 0\n",
    "update_info, eval_info = {}, {}\n",
    "\n",
    "buffer = ReplayBuffer(state_dim, action_dim, buffer_size=cfg.max_timesteps)\n",
    "buffer.distill(d4rl.qlearning_dataset(env), cfg.env, cfg.sample_method, cfg.sample_ratio)\n",
    "\n",
    "policy.alpha = cfg.alpha_finetune\n",
    "policy.pretrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy: RandomizedEnsembles_BC,\n",
    "                    env_name: str,\n",
    "                    seed=cfg.seed,\n",
    "                    eval_episodes=10):\n",
    "    \n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 42)\n",
    "\n",
    "    average_reward, average_length = 0, 0\n",
    "\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "\n",
    "        while not done:\n",
    "            action = policy.act(state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            average_reward += reward\n",
    "            average_length += 1\n",
    "        \n",
    "        average_reward /= eval_episodes\n",
    "        average_length = int(average_length / eval_episodes)\n",
    "\n",
    "        d4rl_score = eval_env.get_normalized_score(average_reward) * 100\n",
    "\n",
    "        return {\n",
    "            \"d4rl\": d4rl_score,\n",
    "            \"evaluation\": average_reward,\n",
    "            \"length\": average_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_return = max_target_returns[cfg.env]\n",
    "with wandb.init(project='adaptive_bc', group=cfg.env, job_type=\"finetune\", name=run_name):\n",
    "    wandb.config.update({k: v for k, v in cfg.__dict__.items() if not k.startswith('__')})\n",
    "\n",
    "    episode_return = 0.0\n",
    "    if cfg.normalize_returns:\n",
    "        last_return = evaluate_policy(policy, cfg.env)[\"evaluation\"] / max_return\n",
    "    else:\n",
    "        last_return = evaluate_policy(policy, cfg.env)[\"d4rl\"] * 0.01\n",
    "        \n",
    "    current_return = last_return\n",
    "    target_return = 1.05\n",
    "\n",
    "    for timestep in tqdm(range(cfg.max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "        \n",
    "        action = (policy.act(state) + np.random.normal(0, scale=cfg.exploration_noise, size=action_dim)).clip(-max_action, max_action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_return += reward\n",
    "\n",
    "        done = float(done) if episode_timesteps < env._max_episode_steps else 0.0\n",
    "        buffer.add_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        for _ in range(cfg.num_updates):\n",
    "            update_info = policy.train(buffer.sample(cfg.batch_size))\n",
    "        \n",
    "        update_info.update({'current_return': current_return, 'last_return': last_return})\n",
    "\n",
    "        wandb.log({'online_training/': update_info,\n",
    "                   'online_trainig/alpha': policy.alpha})\n",
    "        \n",
    "        if done:\n",
    "            state, done = env.reset(), False\n",
    "\n",
    "            if cfg.normalize_returns:\n",
    "                current_return = episode_return / max_return\n",
    "            else:\n",
    "                current_return = env.get_normalized_score(episode_return)\n",
    "            \n",
    "            policy.update_alpha(episode_timesteps, last_return, current_return)\n",
    "\n",
    "            episode_timesteps = 0\n",
    "            episode_return = 0\n",
    "        \n",
    "        if not timestep % cfg.eval_frequency:\n",
    "            eval_info = evaluate_policy(policy, cfg.env, cfg.seed)\n",
    "            wandb.log({'online_evaluation/': eval_info})\n",
    "\n",
    "            if cfg.save_model:\n",
    "                policy.save(f\"online_{run_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
